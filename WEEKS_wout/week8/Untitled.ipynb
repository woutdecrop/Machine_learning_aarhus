{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "class AdaBoostClassifier():\n",
    "\n",
    "    def __init__(self, weak_learner, n_estimators=2):\n",
    "        \"\"\" \n",
    "\n",
    "        Args:\n",
    "          week_learner: weak learner object. Must support fit(self, x, y, weights), and predict(self, X) like sklearn classifiers. \n",
    "          We assume that the week learner predictions are in {-1, 1} as used in AdaBoost \n",
    "\n",
    "          n_estiamtors: int, number of estimators to construct\n",
    "        \"\"\"\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "        self.weak_learner = weak_learner\n",
    "        self.n_estimators = n_estimators\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        AdaBoost Learning Algorithm \n",
    "        \n",
    "        Args:\n",
    "        X: numpy array shape (n,d) - the data (rows)\n",
    "        y: numpy array shape (n,) all elements in {-1, 1} - the labels\n",
    "            \n",
    "        Computes and stores \n",
    "          - models: lists of size n_estimators of weak_learner\n",
    "          - alphas: lists of size n_estimators of float\n",
    "\n",
    "        Returns:\n",
    "          scores: list of scores (accuracy) for each iteration of the algorithm (one value for each model considered)\n",
    "          exp_losses: list of exponential losses for each iteration of the algorithm (one value for each model considered)\n",
    "\n",
    "        to create a weak learners use \n",
    "        tmp = self.weak_learner()\n",
    "        tmp.fit(X, y, p)\n",
    "        \"\"\"  \n",
    "        w = np.ones(X.shape[0])/X.shape[0]\n",
    "        scores = []\n",
    "        exp_losses = []\n",
    "        ### YOUR CODE HERE \n",
    "        for i in range(self.n_estimators):\n",
    "            cur_score = self.score(X, y)\n",
    "            scores.append(cur_score)\n",
    "            exp_loss = self.exp_loss(X, y)\n",
    "            exp_losses.append(exp_loss)\n",
    "            pred = self.ensemble_output(X)\n",
    "            w = np.exp(-y * pred)\n",
    "            p = w/np.sum(w)\n",
    "            wl = self.weak_learner()\n",
    "            wl.fit(X, y, p)\n",
    "            new_pred = wl.predict(X)\n",
    "            correct_idx = (new_pred == y)\n",
    "            wp = np.sum(w[correct_idx])\n",
    "            wn = np.sum(w[~correct_idx])\n",
    "            alphat = (0.5) * np.log(wp/wn)\n",
    "            self.models.append(wl)\n",
    "            self.alphas.append(alphat)\n",
    "            \n",
    "        scores.append(self.score(X, y))\n",
    "        exp_loss = self.exp_loss(X, y)\n",
    "        exp_losses.append(exp_loss)\n",
    "        ### END CODE\n",
    "\n",
    "        # remember to ensure that self.models and self.alphas are filled\n",
    "        assert len(self.models) == self.n_estimators\n",
    "        assert len(self.alphas) == self.n_estimators\n",
    "        return scores, exp_losses\n",
    "        \n",
    "\n",
    "    def exp_loss(self, X, y):\n",
    "        \"\"\" Compute Mean Exponential Loss of the data with the model (1/n sum_i exp(-y_i f(x_i)))\n",
    "\n",
    "        Args:\n",
    "        X: numpy array shape (n, d) - the data (rows)\n",
    "        y: numpy array shape (n,) all elements in {-1, 1} - the labels\n",
    "\n",
    "        Returns:\n",
    "          loss: mean exponential loss\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        ### YOUR CODE here 1-3 lines\n",
    "        pred = self.ensemble_output(X)\n",
    "        loss = np.exp(-y * pred).mean()\n",
    "        ### END CODE\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def ensemble_output(self, X):\n",
    "        \"\"\" Compute the output of the ensemble on the data (sum_i a_i h_i(x_i)) for all data points\n",
    "\n",
    "        Args:\n",
    "        X: numpy array shape (n, d) - the data (rows)\n",
    "        \n",
    "        Returns:\n",
    "          pred: np.array (n, ) ensemble output on each input point in X (rows)\n",
    "        \"\"\"\n",
    "        pred = None\n",
    "        if len(self.models) == 0:\n",
    "            return np.zeros(X.shape[0])\n",
    "        ### YOUR CODE HERE 3-8 lines\n",
    "        all_model_preds = [alpha * model.predict(X) for (alpha, model) in zip(self.alphas, self.models)]\n",
    "        all_model_preds = np.c_[all_model_preds]\n",
    "        pred = np.sum(all_model_preds, axis=0)\n",
    "        ### END CODE\n",
    "        return pred\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" predict function for classifier\n",
    "        Args:\n",
    "          X (numpy array,  shape (n, d)), - the data (rows)\n",
    "        Returns\n",
    "          pred (numpy array,  shape(n,)) values in {-1,+1}, prediction of model on data X - the labels\n",
    "        \"\"\"\n",
    "        pred = None\n",
    "        ### YOUR CODE Here 1-3 lines\n",
    "        pred = np.sign(self.ensemble_output(X))\n",
    "        ### END CODE \n",
    "        return pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Return accuracy of model on data X with labels y ((1/n) (sum_i 1_[f(x_i) == y_i]))\n",
    "        \n",
    "        Args:\n",
    "          X (numpy array shape n, d)\n",
    "        returns\n",
    "          score (float) classifier accuracy on data X with labels y\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        ### YOUR CODE HERE 1-3 lines\n",
    "        pred = self.predict(X)\n",
    "        indicator = pred==y\n",
    "        score = indicator.mean()\n",
    "        ### END CODE\n",
    "        return score\n",
    "\n",
    "        \n",
    "def sklearn_test():\n",
    "   \"\"\" AdaBoost test takes from sklearn\n",
    "   https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\"\"\"\n",
    "\n",
    "   # Construct dataset\n",
    "   x1_samples = 200\n",
    "   x2_samples = 300\n",
    "   X1, y1 = make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=x1_samples, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "   X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=x2_samples, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "   X = np.concatenate((X1, X2))\n",
    "   y = np.concatenate((y1, - y2 + 1))\n",
    "   y = 2*y-1\n",
    "   \n",
    "   # Create and fit an AdaBoosted decision tree\n",
    "   my_learner = lambda: DecisionTreeClassifier(max_depth=1)\n",
    "   bdt = AdaBoostClassifier(my_learner,\n",
    "                            n_estimators=200)\n",
    "\n",
    "   scores, exp_losses = bdt.fit(X, y)\n",
    "   print('Final Accuracy', scores[-1])\n",
    "   fig, ax = plt.subplots(1, 2, figsize=(12,10))\n",
    "   ax[0].plot(exp_losses, 'b--', label='exp loss')\n",
    "   ax[0].plot(1.0 - np.array(scores), 'm--', label='0-1 Loss')\n",
    "   ax[0].legend(fontsize=15)\n",
    "   ax[0].set_title('Loss Per Iteration for AdaBoost', fontsize=20)\n",
    "   \n",
    "   #plot_colors = \"br\"\n",
    "   plot_step = 0.02\n",
    "   #class_names = \"AB\"\n",
    "\n",
    "   # Plot the decision boundaries\n",
    "   x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "   y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "   xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "   Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "   Z = Z.reshape(xx.shape)\n",
    "   cs = ax[1].contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "   ax[1].scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=plt.cm.Paired, edgecolor='k')\n",
    "   ax[1].set_title('AdaBoost Decision Boundary', fontsize=20)\n",
    "   plt.show()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    sklearn_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
