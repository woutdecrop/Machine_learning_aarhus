{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: Break Points and Growth Functions \n",
    "\n",
    "-   Is there always a break point for a finite hypothesis set of $n$\n",
    "    hypotheses? If so, can you give a an upper bound? What is the growth\n",
    "    function?\n",
    "\n",
    "-   Does the set of all functions have a break point? What is its growth\n",
    "    function?\n",
    "\n",
    "-   What is the (smallest) break point for the hypothesis set consisting\n",
    "    of circles centered around $(0,0)$? For a given circle the\n",
    "    hypothesis returns $1$ for points inside the circle and $-1$ for\n",
    "    points outside. What is the growth function?\n",
    "\n",
    "-   What if we move to centered balls in the 3-dimensional space\n",
    "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n",
    "    ${{\\mathbb R}}^d$ (hyperspheres)?\n",
    "\n",
    "-  Show that the growth function for a singleton hypothesis class $H = \\{h\\}$ is 1\n",
    "\n",
    "### SOLUTION MATH\n",
    "1. Since we have $n$ hypotheses, the growth function is capped at $n$. Therefore it must have a break point of at most $\\lfloor \\lg n \\rfloor + 1$.\n",
    "\n",
    "2. It does not have a break point. Its growth function is $m(n)=2^n$ since all functions can create all dichotomies.\n",
    "\n",
    "3. The break point is at 2 since for two points of distinct distances, it is impossible to construct the dichotomy having $1$ on the point furthest from the origin and $-1$ on the point closest. The growth function is the same as positive rays in 1d (sort by distances), hence $m(n)=1+n$.\n",
    "\n",
    "4. The same since all that matters is the distance to the origin.\n",
    "\n",
    "5. For a single point, it is impossible to create both dichotomies since $h$ only gives one fixed output on the point.\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: VC Dimension \n",
    "\n",
    "-   Does VC Dimension depend on the learning algorithm or the actual\n",
    "    data set given?\n",
    "\n",
    "-   Does VC Dimension depend on the probability distribution generating\n",
    "    the data (not the labels)?\n",
    "\n",
    "-   If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n",
    "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n",
    "\n",
    "-   Can you give an upper bound on the VC dimension of a finite set of\n",
    "    $M$ hypotheses?\n",
    "\n",
    "-   What is the VC Dimension for the hypothesis set consisting of\n",
    "    circles centered around 0?\n",
    "\n",
    "-   What if we move to balls (3d)? or in general d dimensions\n",
    "    (hypershperes)?\n",
    "\n",
    "-   What is the maximal VC dimension possible of the intersection of\n",
    "    hypothesis spaces $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n",
    "    dimension $v_1,\\dots,v_n$.\n",
    "\n",
    "-   As previous question, instead what is the minimal VC dimension of\n",
    "    the union of the hypothesis spaces from the previous question\n",
    "\n",
    "-   Show that the VC dimension the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4,\n",
    "    i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 can not.\n",
    "    \n",
    "### SOLUTION MATH\n",
    "1. No\n",
    "2. No\n",
    "3. yes\n",
    "4. yes, $\\lfloor log M \\rfloor$. To see this, notice that for VC dimension $d$, one must be able to shatter a set of $d$ points, i.e. $M \\geq 2^d$.\n",
    "5. same as positive rays in 1d, namely 1. sort by distance -1 +1 is impossible.\n",
    "6. The same\n",
    "7. The minimum of the VC dimensions since the intersection is contained in all.\n",
    "8. The largest of the VC dimensions since you always keep at least the largest and all others might be subset of it.\n",
    "9. To prove at least 4, pick points (0,1), (0,-1), (1,0), (-1,0). Not hard to see that all dichotomies can be obtained. To prove less than 5, consider any set of 5 points in the plane. Computing their bounding box $B$. All the 5 points are containing in $B$, and to construct the \"full\" dichotomy, one must use a rectangle containing at least $B$. Now consider the dichotomy that contains all points on the boundary of the bounding box but none of the points inside. That dichotomy is impossible if the box $B$ has a point in its interior. Finally, if all 5 points lie on the boundary of the bounding box, there are two that lie on the same side of the bounding box. Any dichotomy involving all but one of those two is impossible.\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3: VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n",
    "Consider the input space $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$ (with the first coordinate being the constant 1). Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^{d+1} \\}$ corresponding to the perceptron is $d+1$.\n",
    "\n",
    "We need to show \n",
    "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n",
    "2. That no data set of size d+2 can be shattered by hyperplanes\n",
    "\n",
    "We will give a few more hints than the book does.\n",
    "### Shattering d+1 points\n",
    "As the book hints you must create an \"easy\" data set that you store in matrix $X$. \n",
    "\n",
    "**Hint:** We suggest you consider as a data matrix, the $(d+1) \\times (d+1)$ matrix $X$ whose first column is all-1s (required since $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$) and where the lower $d \\times d$ corner of the matrix is the $d \\times d$ identity matrix.\n",
    "\n",
    "Show that you can construct any dichotomy $y \\in \\{-1,+1\\}^{d+1}$ using some $h \\in \\mathcal{H}$ and the data matrix $X$ defined above.\n",
    "\n",
    "### SOLUTION MATH\n",
    "Given a dichotomy $y \\in \\{-1,+1\\}^{d+1}$, we pick the hypothesis $w$ such that $w_1 = 0.1y_1$ and $w_i = y_i$ for $i>1$. Observe that for the first row of $X$ (data item $x_1$), we have $w^\\intercal x_1 = w_1 = 0.1 y_1$, hence $\\textrm{sign}(w^\\intercal x_1) = y_1$. For $i>1$, notice that $w^\\intercal x_i = w_1 + w_i = 0.1y_1 + y_i$. Since $y_1$ is multiplied by the factor $0.1$, we still have $\\textrm{sign}(w^\\intercal x_i) = y_i$. \n",
    "\n",
    "Here is a cute solution from Nikolai (unfortunately I don't know his last name) from the math group. Nikolai observed that the matrix $X$ is invertable (I find this fact rather obvious. But one can easily show that in any linear combination of 0, the linear factor of the first column must be zero, and after that the remaning columns form an orthogonal basis, meaning the trivial linear combination is the only one). After that, we merely set $w=X^{-1}y$ for an arbitrary dichotomy encoded by $y$. Hence $Xw = XX^{-1}y= y$. Very clean and elegant, in my opinion, and Nikolai's solution deserves some attention. -Chris\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "### No Shattering of d+2 points.\n",
    "Must show that for any d+2 points we must prove there is a  dichotomy hyperplanes can not capture.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Consider an arbitrary set of d+2 points $x_1,\\dots, x_{d+2}$ of dimension (d+1) and think of them as vectors in $\\{1\\} \\times \\mathbb{R}^d \\subset \\mathbb{R}^{d+1}$.\n",
    "Since we have more vectors than dimensions the vectors must be linearly dependent.\n",
    "\n",
    "i.e. \n",
    "$$\n",
    "x_j = \\sum_{i\\neq j} a_i x_i\n",
    "$$\n",
    "Since $x_j$ is determined by the other data points then so is $w^\\intercal x_j$ for any $w$. This means the classification on point $x_j$ is dictated by the classification of the other data points and thus cannot freely be chosen.\n",
    "i.e.\n",
    "$$\n",
    "w^\\intercal x_j = w^\\intercal \\sum_{i\\neq j} a_i x_i =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Define an impossible dichotomy as follows. \n",
    "$$\n",
    "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n",
    "$$\n",
    "Show this dichotomy is impossible!\n",
    "\n",
    "### SOLUTION MATH\n",
    "We have \n",
    "$$\n",
    "w^\\intercal x_j =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Furthermore, we must have $\\textrm{sign}(a_i) = y_i$ for $i \\neq j$ and $\\textrm{sign}(w^\\intercal x_i) = y_i$ for $i \\neq j$. This means that $a_i w^\\intercal x_i$ is positive for $i \\neq j$. Hence $w^\\intercal x_j > 0$ and we cannot have $\\textrm{sign}(w^\\intercal x_j) = -1$.\n",
    "### END SOLUTION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4:  Book Exercises\n",
    "## Exercise 1.11 and 1.12 in the Book \n",
    "(Not problems but exercises inside the text. page 25, 26\n",
    "\n",
    "### SOLUTION MATH\n",
    "1.11\n",
    "a. No. The training data might be such that it contains most examples with the most unlikely label.\n",
    "\n",
    "b. Yes. It might be the case that $p$ is very small, say 0.1, and yet the sample has only points with label +1.\n",
    "\n",
    "c. It equals the probability that $B(n,p)>n/2$ where $n=25$. We can e.g. use Hoeffding to get a bound on this. Let $X \\sim B(n,p)/n$. We see that $\\Pr[|X-p|>1/2-p] \\leq 2\\exp(-2(1/2-p)^2 n) = 2\\exp(-2 \\cdot 0.4^2 \\cdot 25) < 0.00068$. Hence $S$ produces a better hypothesis with probability at least $1-0.00068$.\n",
    "\n",
    "d. No as $S$ outputs the maximum likelihood estimate.\n",
    "\n",
    "A student noticed that Crazy is actually equal to Smart for $p=1/2$. I wasn't considering this case, and think it is nice that she noticed. -Chris\n",
    "\n",
    "1.12\n",
    "a. is not possible as $f$ is unknown.\n",
    "\n",
    "b. not possible. \n",
    "\n",
    "c. Is possible by using a small hypothesis set. If you find a useful hypothesis (low in sample error), you produce that g. Otherwise you report failed.\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Book Problem 2.18 In short\n",
    "Define\n",
    "$$\n",
    "\\mathcal{H}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n",
    "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}\n",
    "$$ \n",
    "\n",
    "Show that the VC dimension of ${{\\mathcal H}}$ is infinite (even though there is only one parameter!)\n",
    "\n",
    "Hint: Use the points set\n",
    "$x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to implement any dichotomy $y_1,\\dots,y_N \\in \\{-1, +1\\}^N$ (find $\\alpha$ that works).\n",
    "You can safely assume $\\alpha >0$.\n",
    "\n",
    "### SOLUTION MATH\n",
    "Map $y$ to $\\alpha$ as follows. define \n",
    "$$\n",
    "f(y) = \\begin{cases}\n",
    "1 \\textrm{ if } y = -1\\\\\n",
    "2 \\textrm{ if } y = +1\n",
    "\\end{cases}\n",
    "$$\n",
    "set $\\alpha$ to the number defined by concatenating the mapped digits $0.f(y_1)f(y_2)\\dots f(y_N)$\n",
    "This means that for $y = [-1, +1, -1]$, $\\alpha = 0.121$.\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n",
    "\n",
    "**Hard:** Can you say something about the changes in behaviour of the optimal solution $w$ as $\\lambda$ decreases from $0$ towards $-\\infty$. When does the optimal cost change from something finite to $-\\infty$?\n",
    "\n",
    "## SOLUTION MATH\n",
    "\n",
    "If we let $\\lambda \\rightarrow \\infty$, then the best choice of $w$ is $0$ as the cost of this solution tends to $\\|y\\|^2$ whereas the cost of all other $w'$ tend to infinity.\n",
    "\n",
    "For $\\lambda<0$, let $\\mu_1 \\geq \\mu_2 \\geq \\dots \\mu_d \\geq 0$ denote the eigenvalues of $X^TX$. If $n\\lambda + \\mu_d > 0$ then the optimal solution can be found as we normaly do. Compute the gradient:\n",
    "$$\n",
    "\\nabla_w = (1/n)(2X^TXw-2X^Ty)+2\\lambda w\n",
    "$$\n",
    "And set to $0$:\n",
    "$$\n",
    "w = (X^TX-n\\lambda I)^{-1}X^Ty\n",
    "$$\n",
    "Notice that $(X^TX-n\\lambda I)$ is invertible since all of its eigenvalues are at least $\\mu_d - n \\lambda > 0$.\n",
    "If $n \\lambda + \\mu_d  <0$ then let $x$ be the unit-length eigenvector of $X^TX$ corresponding to the eigenvalue $\\mu_d$. Let $w = cx$ and consider\n",
    "$$\n",
    "\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2 = \\frac{1}{n}(\\|Xw\\|^2 + \\|y\\|^2 - 2 \\langle Xw,y \\rangle) + \\lambda c^2\n",
    "$$\n",
    "By Cauchy-Schwartz this is no more than\n",
    "$$\n",
    "\\frac{1}{n}(c^2\\mu_d + \\|y\\|^2 + 2 \\|Xw\\|\\|y\\|) + \\lambda c^2 = \\frac{1}{n}(c^2\\mu_d + \\|y\\|^2 + 2 c \\|y\\|) + \\lambda c^2\n",
    "$$\n",
    "We write this as:\n",
    "$$\n",
    "c^2(\\lambda + \\mu_d/n) + (\\|y\\|^2 + 2c\\|y\\|)/n\n",
    "$$\n",
    "The term $\\lambda + \\mu_d/n$ is less than $0$ and $\\|y\\|$ is fixed. This means that as $c \\rightarrow \\infty$, the cost tends to $-\\infty$.\n",
    "\n",
    "Here is an alternative solution. We have the upper bound $\\|Xw-y\\|^2 \\leq 2\\|Xw\\|^2+2\\|y\\|^2 \\leq \\max \\lambda_i(X^TX)\\cdot 2\\|w\\|^2 + 2\\|y\\|^2.$ So for $\\lambda< - 2\\cdot(\\|y\\|^2+\\max \\lambda_i(X^TX))$, $\\lim_{\\|w\\|\\rightarrow \\infty} \\frac{\\|Xw-y\\|^2}{|\\lambda| \\cdot \\|w\\|^2} < 1$ and hence $\\lim_{\\lambda\\rightarrow -\\infty}\\lim_{\\|w\\|\\rightarrow \\infty} \\frac{\\|Xw-y\\|^2}{|\\lambda| \\cdot \\|w\\|^2} = 0$. Essentially the same idea as the solution above. -Chris\n",
    "\n",
    "## END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 7: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise you must we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the in grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the gridsearch module test two different values for max_depth for a a decision tree for wine classification\n",
    "\n",
    "Your job is to good hyperparameters for decision trees for the breast cancer detection.\n",
    "\n",
    "### Task 1:\n",
    "For the breast cancer data set find the best (or very good) combination of max_depth and  min_samples_split  (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements it many not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "### Task 2:\n",
    "- How long time does it take to use grid search validation for $k$ hyperparamers where we test each parameter for $d$ values, and the training algorithm uses f(n) time to train on n data points where we split the data into 5 parts.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 30}</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.865169</td>\n",
       "      <td>0.034490</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.062196</td>\n",
       "      <td>2</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.679708</td>\n",
       "      <td>0.009136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1       0.000590      0.000053         0.000255        0.000001   \n",
       "0       0.000439      0.000035         0.000252        0.000009   \n",
       "\n",
       "  param_max_depth             params  split0_test_score  split1_test_score  \\\n",
       "1              30  {'max_depth': 30}           0.850000           0.833333   \n",
       "0               1   {'max_depth': 1}           0.566667           0.583333   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "1           0.913793         0.865169        0.034490                1   \n",
       "0           0.706897         0.617978        0.062196                2   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "1            1.000000            1.000000            1.000000   \n",
       "0            0.677966            0.669492            0.691667   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "1          1.000000         0.000000  \n",
       "0          0.679708         0.009136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found {'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    \n",
    "w_data = load_wine()\n",
    "wine_data = w_data.data\n",
    "wine_labels = w_data.target\n",
    "\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': [1, 30]}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(wine_data, wine_labels)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.004088</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 2}</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.920914</td>\n",
       "      <td>0.018878</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994723</td>\n",
       "      <td>0.997361</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.993853</td>\n",
       "      <td>0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 10}</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.920914</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984169</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>0.987698</td>\n",
       "      <td>0.003288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 5}</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.952632</td>\n",
       "      <td>0.899471</td>\n",
       "      <td>0.920914</td>\n",
       "      <td>0.022906</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.993846</td>\n",
       "      <td>0.002491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 2}</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.919156</td>\n",
       "      <td>0.019976</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 5}</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.952632</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.917399</td>\n",
       "      <td>0.025279</td>\n",
       "      <td>5</td>\n",
       "      <td>0.986807</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>0.988578</td>\n",
       "      <td>0.002479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 10}</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.952632</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.917399</td>\n",
       "      <td>0.025279</td>\n",
       "      <td>5</td>\n",
       "      <td>0.989446</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.992105</td>\n",
       "      <td>0.991212</td>\n",
       "      <td>0.001249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 10}</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.910053</td>\n",
       "      <td>0.917399</td>\n",
       "      <td>0.030550</td>\n",
       "      <td>5</td>\n",
       "      <td>0.989446</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.992105</td>\n",
       "      <td>0.991212</td>\n",
       "      <td>0.001249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 2}</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.883598</td>\n",
       "      <td>0.908612</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 5}</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.896309</td>\n",
       "      <td>0.036846</td>\n",
       "      <td>9</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.993846</td>\n",
       "      <td>0.002491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.004088      0.000395         0.000340        0.000117   \n",
       "2       0.003262      0.000288         0.000207        0.000004   \n",
       "4       0.003442      0.000368         0.000211        0.000004   \n",
       "6       0.003804      0.000583         0.000285        0.000069   \n",
       "1       0.003765      0.000250         0.000242        0.000010   \n",
       "5       0.003425      0.000399         0.000209        0.000002   \n",
       "8       0.003505      0.000491         0.000208        0.000003   \n",
       "3       0.003433      0.000379         0.000209        0.000001   \n",
       "7       0.003638      0.000433         0.000255        0.000035   \n",
       "\n",
       "  param_max_depth param_min_samples_split  \\\n",
       "0               5                       2   \n",
       "2               5                      10   \n",
       "4              10                       5   \n",
       "6              15                       2   \n",
       "1               5                       5   \n",
       "5              10                      10   \n",
       "8              15                      10   \n",
       "3              10                       2   \n",
       "7              15                       5   \n",
       "\n",
       "                                       params  split0_test_score  \\\n",
       "0    {'max_depth': 5, 'min_samples_split': 2}           0.910526   \n",
       "2   {'max_depth': 5, 'min_samples_split': 10}           0.900000   \n",
       "4   {'max_depth': 10, 'min_samples_split': 5}           0.910526   \n",
       "6   {'max_depth': 15, 'min_samples_split': 2}           0.905263   \n",
       "1    {'max_depth': 5, 'min_samples_split': 5}           0.894737   \n",
       "5  {'max_depth': 10, 'min_samples_split': 10}           0.894737   \n",
       "8  {'max_depth': 15, 'min_samples_split': 10}           0.884211   \n",
       "3   {'max_depth': 10, 'min_samples_split': 2}           0.894737   \n",
       "7   {'max_depth': 15, 'min_samples_split': 5}           0.894737   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.947368           0.904762         0.920914        0.018878   \n",
       "2           0.957895           0.904762         0.920914        0.026256   \n",
       "4           0.952632           0.899471         0.920914        0.022906   \n",
       "6           0.947368           0.904762         0.919156        0.019976   \n",
       "1           0.952632           0.904762         0.917399        0.025279   \n",
       "5           0.952632           0.904762         0.917399        0.025279   \n",
       "8           0.957895           0.910053         0.917399        0.030550   \n",
       "3           0.947368           0.883598         0.908612        0.027815   \n",
       "7           0.942105           0.851852         0.896309        0.036846   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                1            0.994723            0.997361   \n",
       "2                1            0.984169            0.992084   \n",
       "4                1            0.992084            0.992084   \n",
       "6                4            1.000000            1.000000   \n",
       "1                5            0.986807            0.992084   \n",
       "5                5            0.989446            0.992084   \n",
       "8                5            0.989446            0.992084   \n",
       "3                8            1.000000            1.000000   \n",
       "7                9            0.992084            0.992084   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "0            0.989474          0.993853         0.003278  \n",
       "2            0.986842          0.987698         0.003288  \n",
       "4            0.997368          0.993846         0.002491  \n",
       "6            1.000000          1.000000         0.000000  \n",
       "1            0.986842          0.988578         0.002479  \n",
       "5            0.992105          0.991212         0.001249  \n",
       "8            0.992105          0.991212         0.001249  \n",
       "3            1.000000          1.000000         0.000000  \n",
       "7            0.997368          0.993846         0.002491  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found {'max_depth': 5, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "cancer_data = load_breast_cancer()\n",
    "c_data = cancer_data.data\n",
    "c_labels = cancer_data.target\n",
    "\n",
    "\n",
    "def decisiontree_model_selection(train_data, labels):\n",
    "    clf = None\n",
    "    ### YOUR CODE HERE\n",
    "    reg_parameters = {'max_depth': [5, 10, 15],\n",
    "                      'min_samples_split': [2, 5, 10]}                \n",
    "    clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "    clf.fit(train_data, labels)\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(c_data, c_labels)\n",
    "bt = show_result(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
