{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 7 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Convolution and Pooling\n",
    "In this exercise we will see if we can get a better grasp of convolution and pooling.\n",
    "Your job is to implement basic convolution and pooling.\n",
    "\n",
    "For this we need the python package pillow  which you need to install\n",
    "\n",
    "- The Convolution operator that takes a $d \\times d$ weight matrix and 1 channel image, and applies the convolution with the weight vector with the image.\n",
    "- The max pooling operator takes an input image and a max pooling size and returns the pooled output.\n",
    "    For simplicity we only consider 2 x 2 max pooling\n",
    "- Test your convolution implementation with the $3 \\times 3$ matrix with -1 everywhere exept the middle and 8 in the middle is a classic edge detector pattern.\n",
    "- Test your pooling implementation by applying a $2 \\times 2$ max pooling to the output of the convolution.\n",
    "\n",
    "We assume that we pad the input to ensure the output of the convolution is the same width and height as the input image.\n",
    "\n",
    "To compare your implementation we have supplied code that applies the convolution and the pooling operator from the neural net package in pytorch.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lena - fint better image\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image #(pacakge name is pillow - i.e. pip3 install pillow )\n",
    "import os\n",
    "filename = 'lena512_gray.bmp'\n",
    "\n",
    "img = Image.open('lena512_gray.bmp')\n",
    "lena = np.array(img)\n",
    "print('image shape', lena.shape)\n",
    "plt.imshow(lena, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "def conv2d(img, w):\n",
    "    \"\"\" Return the result of applying the convolution defined by w to img - \n",
    "    for simplicity assume that w is square\"\"\"\n",
    "    w_dim = w.shape[0]\n",
    "    pad = w_dim - 2\n",
    "    padded_img = np.pad(img, [pad, pad], 'constant', constant_values=0)    \n",
    "    out = np.zeros(img.shape)\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "def max_pool2d(img):\n",
    "    \"\"\" Return the result of applying the 2 x 2 max pooling operator to mig (halve the width and height of image)\"\"\"\n",
    "    out = np.zeros((int(img.shape[0]/2), int(img.shape[1]/2)))\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return out\n",
    "\n",
    "conv_filter = np.array([[-1., -1., -1.], [-1., 8, -1.], [-1., -1., -1.]])\n",
    "convoluted_lena = conv2d(lena, conv_filter)\n",
    "pooled_lena = max_pool2d(convoluted_lena)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 16))\n",
    "axes[0].imshow(convoluted_lena, cmap='gray', vmin=convoluted_lena.min(), vmax=convoluted_lena.max())\n",
    "axes[1].imshow(pooled_lena, cmap='gray', vmin=convoluted_lena.min(), vmax=convoluted_lena.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch_lena = torch.from_numpy(lena).view(1, 1, lena.shape[0], lena.shape[1]).double()\n",
    "print('image shape', torch_lena.shape)\n",
    "tv = torch.tensor([[-1., -1., -1.], [-1., 8, -1.], [-1., -1., -1.]])\n",
    "tv = tv.view(1, 1, 3, 3).double()\n",
    "torch_convoluted_lena = F.conv2d(torch_lena, tv, torch.tensor([0.], dtype=torch.double), 1, 1, 1, 1)\n",
    "print('convoluted_lena shape', torch_convoluted_lena.shape)\n",
    "print('conv diff norm', np.linalg.norm(torch_convoluted_lena.numpy().squeeze() - convoluted_lena))\n",
    "torch_pooled_lena = F.max_pool2d(torch_convoluted_lena, kernel_size=(2, 2))\n",
    "print('pool diff norm', np.linalg.norm(torch_pooled_lena.numpy().squeeze() - pooled_lena))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 16))\n",
    "axes[0].imshow(torch_convoluted_lena.numpy().squeeze(), cmap='gray', vmin=torch_convoluted_lena.min(), vmax=torch_convoluted_lena.max())\n",
    "axes[1].imshow(torch_pooled_lena.numpy().squeeze(), cmap='gray', vmin=torch_convoluted_lena.min(), vmax=torch_convoluted_lena.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: How Many Parameters Does the VGG net use\n",
    "The VGG net is a well known neural net architecture for image recognition. \n",
    "\n",
    "**How many parameters does it use? Write down a formula and compute the number!**\n",
    "\n",
    "It has the following architecture [https://pytorch.org/docs/0.4.0/_modules/torchvision/models/vgg.html]\n",
    "\n",
    "It takes as input an image of size 224 x 224 x 3.\n",
    "\n",
    "First it computes 64, 3 x 3 x 3 convolutions i.e. makes 64 channels. Then it makes 64, 3 x 3 x 64 convolutions on the output of the previous convolution i.e. it makes 64 new channels. This is followd by  a $2 \\times 2$ max pooling operation.\n",
    "\n",
    "The pattern continues as described below: A number is the number of new convolutions made on previous input and 'M' means max pooling.\n",
    "\n",
    "64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'\n",
    "\n",
    "All convolutions are 3x3 using padding of 1 of zeros around the input so the convolution output size has height and width the same as the input.\n",
    "All poolings are $2 \\times 2$ non-overlapping (stride=2)\n",
    "\n",
    "After the fully convolutional layers it used standard fully connected layers as\n",
    "- 512 * 7 * 7, 4096 (why 512 * 7 * 7?)\n",
    "- 4096, 4096,\n",
    "- 4096, 1000\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3:  Bias Variance \n",
    "\n",
    "-   Does Bias and Variance terms (two numbers) in the Bias Variance\n",
    "    decomposition depend on the learning algorithm.\n",
    "\n",
    "-   What is Variance (in Bias Variance tradoff) if we have a hypothesis\n",
    "    set of size $1$ namely the constant model $h(x) = 2$. The learning\n",
    "    algorithm always picks this hypothesis no matter the data.\n",
    "\n",
    "-   What is the Variance (in the Bias Variance tradeoff) if the simple\n",
    "    hypothesis from the previous question is replaced by a very very\n",
    "    sophisticated hypothesis.\n",
    "\n",
    "-   Assume the target function is a second degree polynomial, and the\n",
    "    input to your algorithm is always eleven distinct (noiseless) points. Your\n",
    "    hypothesis set is the set of all degreee 10 polynomial and the\n",
    "    learning algorithm returns the hypothesis with the best fit\n",
    "    (miniming least squared error) given the data. What is Bias and what\n",
    "    is Variance?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Bias Variance - Hard Exercise\n",
    "Book Problem 2.24 part (a)\n",
    "\n",
    "Short Version:\n",
    "   \n",
    "  - The target function is $f(x) = x^2$ and the cost is Least Squares.\n",
    "\n",
    "  - Sample two points $x_1, x_2$ from $[-1, 1]$ uniformly at random to get the data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$\n",
    "\n",
    "  - Use hypothesis space $\\{h(x) = ax +b\\mid a,b\\in\\Bbb R\\}$ i.e. lines. There are two parameters $a$ and $b$.\n",
    "\n",
    "  - Given a data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$ the algorithm returns the line that fits these points.\n",
    "\n",
    "  - Your task is to write down an analytical expression for $\\bar{g} = \\mathbb{E}_D [h_D]$ where $h_D$ is the hypothesis learned on D.\n",
    "\n",
    "**Step 1.** What is the in sample error of $h_D$ and why?\n",
    "\n",
    "**Step 2.** Given $D$ what are $a, b$ (defined by the line between $(x_1, x_1^2)$ and  $(x_2, x_2^2)$)? Hint: $x_2^2- x_1^2 = (x_2-x_1)(x_2 + x_1)$.\n",
    "\n",
    "**Step 3.** What is the expected value of the slope $a$ over $x_1$ and $x_2$?\n",
    "\n",
    "**Step 4.** What is the expected value of the intercerpt $b$ over $x_1$ and $x_2$? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**More hints**\n",
    "For the uniform distribution over $[-1,1]$ the mean is $0$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Bias Variance Experiment \n",
    "In this exercise you must redo the experiment shown at the lectures.\n",
    "This exercise takes up quite a lot of space so we have moved it to a separate notebook. Go to [BiasVariance Notebook](BiasVariance.ipynb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
